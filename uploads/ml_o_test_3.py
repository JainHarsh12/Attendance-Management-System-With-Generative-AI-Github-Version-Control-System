# 1. Explain the bias-variance tradeoff. How does it affect model performance?
# 2. What are different regularization techniques in machine learning?
# 3. What is cross-validation, and why is it important in machine learning?
# 4. Explain the difference between bagging and boosting in ensemble learning.
# 5. What are the key assumptions of linear regression?
# 6. How does a decision tree determine the best split at each node?
# 7. What is the Curse of Dimensionality in machine learning?
# 8. What is the difference between a Random Forest and a single decision tree?
# 9. What is the difference between a parametric and non-parametric model in machine learning?
# 10. Explain the concept of feature importance. How is it measured in tree-based models like Random Forest or Gradient Boosting?
# 11. What is gradient descent, and what are its variants (e.g., stochastic, mini-batch)?
# 12. How does k-Nearest Neighbors (k-NN) work, and what are its limitations?
# 13. What are the pros and cons of using SVM (Support Vector Machine) for classification?
# 14. Explain the differences between logistic regression and linear regression.
# 15. How does gradient boosting work? What makes it different from other boosting methods like AdaBoost?